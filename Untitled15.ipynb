{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled15.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtU/ZmZkVpgH5Y7uo6nyCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sid-electrical/mycaptain/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IYDO1kBBEi0"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "# -11,23 +9,15 @@\n",
        "import os\n",
        "import h5py\n",
        "\n",
        "# fixed-sizes for image\n",
        "fixed_size = tuple((500, 500))\n",
        "\n",
        "# path to training data\n",
        "train_path = \"dataset/train\"\n",
        "\n",
        "# no.of.trees for Random Forests\n",
        "num_trees = 100\n",
        "\n",
        "# bins for histogram\n",
        "bins = 8\n",
        "\n",
        "# train_test_split size\n",
        "test_size = 0.10\n",
        "\n",
        "# seed for reproducing same results\n",
        "seed = 9\n",
        "#--------------------\n",
        "# tunable-parameters\n",
        "#--------------------\n",
        "images_per_class = 80\n",
        "fixed_size       = tuple((500, 500))\n",
        "train_path       = \"dataset/train\"\n",
        "h5_data          = 'output/data.h5'\n",
        "h5_labels        = 'output/labels.h5'\n",
        "bins             = 8\n",
        "\n",
        "# feature-descriptor-1: Hu Moments\n",
        "def fd_hu_moments(image):\n",
        "# -64,13 +54,7  def fd_histogram(image, mask=None):\n",
        "\n",
        "# empty lists to hold feature vectors and labels\n",
        "global_features = []\n",
        "labels = []\n",
        "\n",
        "i, j = 0, 0\n",
        "k = 0\n",
        "\n",
        "# num of images per class\n",
        "images_per_class = 80\n",
        "labels          = []\n",
        "\n",
        "# loop over the training data sub-folders\n",
        "for training_name in train_labels:\n",
        "#-80,7 +64,6  def fd_histogram(image, mask=None):\n",
        "    # get the current training label\n",
        "    current_label = training_name\n",
        "\n",
        "    k = 1\n",
        "    # loop over the images in each sub-folder\n",
        "    for x in range(1,images_per_class+1):\n",
        "        # get the image file namedef fd_histogram(image, mask=None):\n",
        "        labels.append(current_label)\n",
        "        global_features.append(global_feature)\n",
        "\n",
        "        i += 1\n",
        "        k += 1\n",
        "    print \"[STATUS] processed folder: {}\".format(current_label)\n",
        "    j += 1\n",
        "    print(\"[STATUS] processed folder: {}\".format(current_label))\n",
        "\n",
        "print \"[STATUS] completed Global Feature Extraction...\"\n",
        "print(\"[STATUS] completed Global Feature Extraction...\")\n",
        "\n",
        "# get the overall feature vector size\n",
        "print \"[STATUS] feature vector size {}\".format(np.array(global_features).shape)\n",
        "print(\"[STATUS] feature vector size {}\".format(np.array(global_features).shape))\n",
        "\n",
        "# get the overall training label size\n",
        "print \"[STATUS] training Labels {}\".format(np.array(labels).shape)\n",
        "print(\"[STATUS] training Labels {}\".format(np.array(labels).shape))\n",
        "\n",
        "# encode the target labels\n",
        "targetNames = np.unique(labels)\n",
        "le = LabelEncoder()\n",
        "target = le.fit_transform(labels)\n",
        "print \"[STATUS] training labels encoded...\"\n",
        "le          = LabelEncoder()\n",
        "target      = le.fit_transform(labels)\n",
        "print(\"[STATUS] training labels encoded...\")\n",
        "\n",
        "# normalize the feature vector in the range (0-1)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scale features in the range (0-1)\n",
        "scaler            = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled_features = scaler.fit_transform(global_features)\n",
        "print \"[STATUS] feature vector normalized...\"\n",
        "print(\"[STATUS] feature vector normalized...\")\n",
        "\n",
        "print \"[STATUS] target labels: {}\".format(target)\n",
        "print \"[STATUS] target labels shape: {}\".format(target.shape)\n",
        "print(\"[STATUS] target labels: {}\".format(target))\n",
        "print(\"[STATUS] target labels shape: {}\".format(target.shape))\n",
        "\n",
        "# save the feature vector using HDF5\n",
        "h5f_data = h5py.File('output/data.h5', 'w')\n",
        "h5f_data = h5py.File(h5_data, 'w')\n",
        "h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))\n",
        "\n",
        "h5f_label = h5py.File('output/labels.h5', 'w')\n",
        "h5f_label = h5py.File(h5_labels, 'w')\n",
        "h5f_label.create_dataset('dataset_1', data=np.array(target))\n",
        "\n",
        "h5f_data.close()\n",
        "h5f_label.close()\n",
        "\n",
        "print \"[STATUS] end of training..\" \n",
        "print(\"[STATUS] end of training..\") \n",
        " 79  organize_flowers17.py \n",
        "\n",
        "#-----------------------------------------\n",
        "# DOWNLOAD AND ORGANIZE FLOWERS17 DATASET\n",
        "#-----------------------------------------\n",
        "import os\n",
        "import glob\n",
        "import datetime\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "def download_dataset(filename, url, work_dir):\n",
        "\tif not os.path.exists(filename):\n",
        "\t\tprint(\"[INFO] Downloading flowers17 dataset....\")\n",
        "\t\tfilename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
        "\t\tstatinfo = os.stat(filename)\n",
        "\t\tprint(\"[INFO] Succesfully downloaded \" + filename + \" \" + str(statinfo.st_size) + \" bytes.\")\n",
        "\t\tuntar(filename, work_dir)\n",
        "\n",
        "def jpg_files(members):\n",
        "\tfor tarinfo in members:\n",
        "\t\tif os.path.splitext(tarinfo.name)[1] == \".jpg\":\n",
        "\t\t\tyield tarinfo\n",
        "\n",
        "def untar(fname, path):\n",
        "\ttar = tarfile.open(fname)\n",
        "\ttar.extractall(path=path, members=jpg_files(tar))\n",
        "\ttar.close()\n",
        "\tprint(\"[INFO] Dataset extracted successfully.\")\n",
        "\n",
        "#-------------------------\n",
        "# MAIN FUNCTION\n",
        "#-------------------------\n",
        "if __name__ == '__main__':\n",
        "\tflowers17_url  = \"http://www.robots.ox.ac.uk/~vgg/data/flowers/17/\"\n",
        "\tflowers17_name = \"17flowers.tgz\"\n",
        "\ttrain_dir      = \"dataset\"\n",
        "\n",
        "\tif not os.path.exists(train_dir):\n",
        "\t\tos.makedirs(train_dir)\n",
        "\n",
        "\tdownload_dataset(flowers17_name, flowers17_url, train_dir)\n",
        "\tif os.path.exists(train_dir + \"\\\\jpg\"):\n",
        "\t\tos.rename(train_dir + \"\\\\jpg\", train_dir + \"\\\\train\")\n",
        "\n",
        "\n",
        "\t# get the class label limit\n",
        "\tclass_limit = 17\n",
        "\n",
        "\t# take all the images from the dataset\n",
        "\timage_paths = glob.glob(train_dir + \"\\\\train\\\\*.jpg\")\n",
        "\n",
        "\t# variables to keep track\n",
        "\tlabel = 0\n",
        "\ti = 0\n",
        "\tj = 80\n",
        "\n",
        "\t# flower17 class names\n",
        "\tclass_names = [\"daffodil\", \"snowdrop\", \"lilyvalley\", \"bluebell\", \"crocus\",\n",
        "\t\t\t   \t   \"iris\", \"tigerlily\", \"tulip\", \"fritillary\", \"sunflower\", \n",
        "\t\t\t       \"daisy\", \"coltsfoot\", \"dandelion\", \"cowslip\", \"buttercup\",\n",
        "\t\t\t       \"windflower\", \"pansy\"]\n",
        "\n",
        "\t# loop over the class labels\n",
        "\tfor x in range(1, class_limit+1):\n",
        "\t\t# create a folder for that class\n",
        "\t\tos.makedirs(train_dir + \"\\\\train\\\\\" + class_names[label])\n",
        "\n",
        "\t\t# get the current path\n",
        "\t\tcur_path = train_dir + \"\\\\train\\\\\" + class_names[label] + \"\\\\\"\n",
        "\n",
        "\t\t# loop over the images in the dataset\n",
        "\t\tfor index, image_path in enumerate(image_paths[i:j], start=1):\n",
        "\t\t\toriginal_path   = image_path\n",
        "\t\t\timage_path      = image_path.split(\"\\\\\")\n",
        "\t\t\timage_file_name = str(index) + \".jpg\"\n",
        "\t\t\tos.rename(original_path, cur_path + image_file_name)\n",
        "\n",
        "\t\ti += 80\n",
        "\t\tj += 80\n",
        "\t\tlabel += 1 \n",
        "  71  train_test.py \n",
        "\n",
        "#-----------------------------------\n",
        "# TRAINING OUR MODEL\n",
        "#-----------------------------------\n",
        "\n",
        "# import the necessary packages\n",
        "import h5py\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import warnings\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "@@ -21,59 +20,70 @@\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#--------------------\n",
        "# tunable-parameters\n",
        "#--------------------\n",
        "num_trees = 100\n",
        "test_size = 0.10\n",
        "seed      = 9\n",
        "test_path = \"dataset/test\"\n",
        "h5_data   = 'output/data.h5'\n",
        "h5_labels = 'output/labels.h5'\n",
        "scoring   = \"accuracy\"\n",
        "\n",
        "if not os.path.exists(test_path):\n",
        "    os.makedirs(test_path)\n",
        "\n",
        "# create all the machine learning models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(random_state=9)))\n",
        "models.append(('LR', LogisticRegression(random_state=seed)))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier(random_state=9)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=9)))\n",
        "models.append(('CART', DecisionTreeClassifier(random_state=seed)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC(random_state=9)))\n",
        "models.append(('SVM', SVC(random_state=seed)))\n",
        "\n",
        "# variables to hold the results and names\n",
        "results = []\n",
        "names = []\n",
        "scoring = \"accuracy\"\n",
        "names   = []\n",
        "\n",
        "# import the feature vector and trained labels\n",
        "h5f_data = h5py.File('output/data.h5', 'r')\n",
        "h5f_label = h5py.File('output/labels.h5', 'r')\n",
        "h5f_data  = h5py.File(h5_data, 'r')\n",
        "h5f_label = h5py.File(h5_labels, 'r')\n",
        "\n",
        "global_features_string = h5f_data['dataset_1']\n",
        "global_labels_string = h5f_label['dataset_1']\n",
        "global_labels_string   = h5f_label['dataset_1']\n",
        "\n",
        "global_features = np.array(global_features_string)\n",
        "global_labels = np.array(global_labels_string)\n",
        "global_labels   = np.array(global_labels_string)\n",
        "\n",
        "h5f_data.close()\n",
        "h5f_label.close()\n",
        "\n",
        "# verify the shape of the feature vector and labels\n",
        "print \"[STATUS] features shape: {}\".format(global_features.shape)\n",
        "print \"[STATUS] labels shape: {}\".format(global_labels.shape)\n",
        "print(\"[STATUS] features shape: {}\".format(global_features.shape))\n",
        "print(\"[STATUS] labels shape: {}\".format(global_labels.shape))\n",
        "\n",
        "print \"[STATUS] training started...\"\n",
        "print(\"[STATUS] training started...\")\n",
        "\n",
        "# split the training and testing data\n",
        "(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n",
        "                                                                                          np.array(global_labels),\n",
        "                                                                                          test_size=test_size,\n",
        "                                                                                          random_state=seed)\n",
        "\n",
        "print \"[STATUS] splitted train and test data...\"\n",
        "print \"Train data  : {}\".format(trainDataGlobal.shape)\n",
        "print \"Test data   : {}\".format(testDataGlobal.shape)\n",
        "print \"Train labels: {}\".format(trainLabelsGlobal.shape)\n",
        "print \"Test labels : {}\".format(testLabelsGlobal.shape)\n",
        "\n",
        "# filter all the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"[STATUS] splitted train and test data...\")\n",
        "print(\"Train data  : {}\".format(trainDataGlobal.shape))\n",
        "print(\"Test data   : {}\".format(testDataGlobal.shape))\n",
        "print(\"Train labels: {}\".format(trainLabelsGlobal.shape))\n",
        "print(\"Test labels : {}\".format(testLabelsGlobal.shape))\n",
        "\n",
        "# 10-fold cross validation\n",
        "for name, model in models:\n",
        "    kfold = KFold(n_splits=10, random_state=7)\n",
        "    kfold = KFold(n_splits=10, random_state=seed)\n",
        "    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create the model - Random Forests\n",
        "clf  = RandomForestClassifier(n_estimators=100, random_state=9)\n",
        "clf  = RandomForestClassifier(n_estimators=num_trees, random_state=seed)\n",
        "\n",
        "# fit the training data to the model\n",
        "clf.fit(trainDataGlobal, trainLabelsGlobal)\n",
        "\n",
        "# path to test data\n",
        "test_path = \"dataset/test\"\n",
        "\n",
        "# loop through the test images\n",
        "for file in glob.glob(test_path + \"/*.jpg\"):\n",
        "    # read the image\n",
        "\n",
        "    ###################################\n",
        "    global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n",
        "\n",
        "    # scale features in the range (0-1)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    rescaled_feature = scaler.fit_transform(global_feature)\n",
        "\n",
        "    # predict label of test image\n",
        "    prediction = clf.predict(global_feature.reshape(1,-1))[0]\n",
        "    prediction = clf.predict(rescaled_feature.reshape(1,-1))[0]\n",
        "\n",
        "    # show predicted label on image\n",
        "    cv2.putText(image, train_labels[prediction], (20,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}